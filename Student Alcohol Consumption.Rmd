
---
title: "Student Failure Rates and Alcohol Consumption"
author: "Raymond Peter David"
date: "5/29/2020"
output:
  pdf_document: default
  code_folding: hide
  toc_float: TRUE
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

              "A research into student failures and its potential underlying factors"                 

\newpage
```

                                    Table Of Contents
```

**1.Introduction(Executive Summary)**
    
     1(a).Objectives
     1(b).Hypothesis
     1(c).Preprocessing
     1(d).Initial data exploration



**2.Method and Analysis**
     
     2(a).Brief overview of models used
     2(b).RandomForest model
     2(c).KNN model



**3.Results**
   
    3(a).Random Forest results
    3(b).KNN results
    3(c).Summary of results



**4.Conclusion**
   
    4(a).Summary of findings
    4(b).Potential impact
    4(c).Limitations
    4(d).Future work
    


**5.Credits(Citation)**
    5(a). Dataset owner and conrtibutors

\newpage

**Introduction**

In this project, we will use the data found on kaggle titled "Student Alcohol Consumption" that is created by UCI Machine Learning.This dataset is obtained from a survey of students taking math and portugeese lessons, which we will use to predict and test our hypothesis.There is a combined total of 1044 observation with 34 different variables. We will conduct an eploritary data analysis and create a model to predict the number of student failures in relation to alcohol and other variables present in the dataset.

To get an idea of what each variables meant, you can check the footnotes provided[^1].

**1(a).Objectives** 

The objective of this project is to conduct a research to evaluate student performance in school based on the relationship between daily alchol consumption and student failures. Also, we will try to explore which variables are closely related to each other and also find out which variables have the biggest influence on student failures. 

**1(b).Hypothesis**



*Daily alcohol consumption will not be the most influential determining factor of students failure.

*Parents Cohabitation (living apart) will be a strong factor for students failure.

*Travel time will not influence student failures but reason to go to school will. 





**1(c).Preprocessing**
```{r preprocessing,results='hide'}
##Install and Load packages
if (!require(tidyverse)) install.packages('tidyverse')
library(tidyverse)
if (!require(randomForest)) install.packages('randomForest')
library(randomForest)
if (!require(kknn)) install.packages('kknn')
library(kknn)
if (!require(plyr)) install.packages('plyr')
library(plyr) # to combine dataset
if (!require(dplyr)) install.packages('dplyr')
library(dplyr)
if (!require(caret)) install.packages('caret')
library(caret) # to evaluate model
if (!require(Boruta)) install.packages('Boruta')
library(Boruta) #variable selection to improve model
```

```{r load_files}
d1 <- read_csv("~/Downloads/datasets_251_561_student-mat.csv")
#add new column class Math
d1<-data.frame(Class="Math",d1)
d2 <- read_csv("~/Downloads/datasets_251_561_student-por.csv")
#add new column class Math
d2<-data.frame(Class="Port",d2)
#BIND
d3<-rbind.fill(d1,d2)
```

Since the number of failures are reported with numbers from 0-4, we will make it simpler by annotaing the number 1 to resemble students that fails and 0 for students that don't fail.

```{r}
#create binary data, FAIL or NOT (dichotomic approach)
d3$failures<-ifelse(d3$failures==0,0,1)%>%
  as.factor
```
\newpage

**1(d).Intial data exploration**
#Checking for missing values 
One of the most important step before doing anything else is to check for missing values.If missing values are present, it should be removed. In this dataset, we see that there is no missing values.
```{r}
sum(is.na(d3))
```

```{r}
#Check data structure
str(d3)
```
\newpage
```{r}
#Check variable names 
names(d3)
#Top 6 data
head(d3)
```

#Method and Analysis

**2(a).Brief overview of model used**

To reach our objectives, we will tackle 2 different approaches: random forests and K nearest neighbor.

#Random forest
To know how random forest works, we should first understand the concept of decision trees. Decision trees are used to create prediction over an event or to test the characteristic that we want. Random forest works by combining the predictions of large numbers of individual decision trees. Generally, the higher the amount of trees, the more accurate the model will be. Although some of the individual trees may be incorrect, there will be more correct trees that still produces good results.To make it simpler, random forest is used to create a prediction that is more accurate than random guessing[^2].However, it is important to note that we should try to find the best number of trees which will be suitable for our dataset to be accurate. 


#K nearest neighbor
K nearest neighbor (KNN), predicts value of datapoints  according to the assigned value(K).It is generally conducted to compare the resemblance of our predictions to the training data that we decided in advanced[^3].

Based on these two models, we can't really decide on the best or better model. Both the KNN and random forest has their own approach which may work better for one type of activity,say classification, but not as good for another. However, in general, random forest is more widely used due to the time consuming activity of the KNN model. 


**2(b).RandomForest model**
```{r random_forest}
set.seed(123,sample.kind = "Rounding")
# if using R 3.5 or earlier, use `set.seed(123)` instead
rf0 = randomForest(failures ~., # The model
                   data=d3, # The dataset
                   mtry=2) # Hyperparameter mtry
#Which variable make the model became better?
##The bor function will help us decide which variable to use, so that our model is maximised. If the variable ##will maximise our model, it will return the value as 'confirmed'.If not, it will either return a 'tentative' or ##'rejected'.
set.seed(123,sample.kind = "Rounding")
bor = bor = Boruta(failures ~., data=d3)
```

```{r}
bor$finalDecision[which(bor$finalDecision=="confirmed")]
```
```
#CONFIRMED VARIABLES
|   Class  |    age   |   Medu   |   Fedu   |   Mjob   | guardian |
|----------|----------|----------|----------|----------|----------|
|confirmed |confirmed |confirmed |confirmed |confirmed |confirmed |
| studytime|  paid    |  higher  |  famrel  |   Dalc   | absences |
|----------|----------|----------|----------|----------|----------|
|confirmed |confirmed |confirmed |confirmed |confirmed |confirmed |
|    G1    |    G2    |    G3    |
|----------|----------|----------|
|confirmed |confirmed |confirmed |
```
\newpage
```
#Rejected Variables
|  school  |    sex   |  address | famsize  | Pstatus  |
|----------|----------|----------|----------|----------|
| rejected | rejected | rejected | rejected | rejected |
|  Fjob    |  reason  |traveltime| schoolsup|  famsup  | 
|----------|----------|----------|----------|----------|
| rejected | rejected | rejected | rejected | rejected |
|   goout  |    Walc  |  health  |
|----------|----------|----------|
| rejected | rejected | rejected |
```
```{r}
set.seed(123,sample.kind = "Rounding")
# Using the variables selected by Boruta algorithm
rf_bor <- randomForest(failures ~ Class + age + Medu + Fedu + Mjob + guardian + studytime + paid + higher + famrel + Dalc + absences + G1 + G2 + G3,
                       data = d3)
```

```{r}
# Checking the model
varImpPlot(rf_bor)
```
Looking at the plot above, we can see how G3,G1,and G2 are in the top 5 most important variables to determine failures.
Lets dig into the data deeper.
```{r}
#Boxplot of G1,G2,G3 against failures
ggplot(d3) +
aes(x = failures, y = G1) +
geom_boxplot(fill = "#0c4c8a") +
theme_minimal()
```
\newpage
```{r}
ggplot(d3) +
aes(x = failures, y = G2) +
geom_boxplot(fill = "#0c4c8a") +
theme_minimal()
```
\newpage
```{r}
ggplot(d3) +
aes(x = failures, y = G3) +
geom_boxplot(fill = "#0c4c8a") +
theme_minimal()
```

G1:first period grade
G2:second period grade
G3:final grade
Looking at these 3 variables and the boxplot, we don't get anything other than the obvious. The higher the score (G1,G2,G3), the lesser the number of failures. The lower the score, the higher number of failures occurs. 

\newpage
## Auxiliar Plots
```{r}
# Checking correlation between G1, G2, G3
pairs(d3[,c("G1","G2","G3")])
```

Based on the auxiliar plot, we can see that G1, G2, and G3 are highly correlated.Therefore, it might be a good idea to just combine these variables into one for the sake of clarity and observation. 


```{r}
# Joining the 3 variables by average grade
d4 <- mutate(d3, avg_Grade = (d3$G1 + d3$G2 + d3$G3)/3)
d4 <- select(d4, !c(G1,G2,G3))
```

After joining these variables, we will try to see if these changes the accuracy of our model.

```{r}
# In this case, its nothing better...
set.seed(123,sample.kind = "Rounding")
rf_bor2 <- randomForest(failures ~ Class + age + Medu + Fedu + Mjob + guardian + studytime + paid + higher + famrel + Dalc + absences + avg_Grade,
                       data = d4)
 
```

```{r}
#checking for error rate:
rf_bor
rf_bor2
```


Looking back at the rf_bor plot, we can get more useful insights than just looking at the average grades. 
```{r}
varImpPlot(rf_bor)
```

Ignoring G1,G2,and G3, we can see that the best variable that determines failure is age. From this we can deduced that as the age of the students increase, so is the number of failures.This makes sense since as the student grow older, they are more vulnerable to all kinds of different things. But, does this imply that age is more important than alcohol to determine children failure? For example, the students might reached their legal age which will increase the chance of consuming alcohol. And as we can see, daily alcohol intake(Dalc) is also one of the  determinant of student failures.However, it is wrong to say that this is all caused by alcohol consumption since as the students grow older, they will more likely to be in a relationship and party more, which may also cause an increase in failure rates. However, this data can be useful for the school since if age is really the most important factor, then the school must apply some strategies to deal with this problem such as adding more extra classes for the older students. 

```{r}
ggplot(d3) +
 aes(x = failures, y = age) +
 geom_boxplot(fill = "#0c4c8a") +
 theme_minimal()
```

Next,we can see that absences are on a higher rank than alcohol consumption. The higher the number of absences, the higher the number of failures. This is easily explained because once a student is absent from class, he or she has a lot to catch up and as the number of absents increased, the workload will pile up. Hence, it might cause the students to be behind of his or her classmates and also increase the chances of failing.Therefore, the school must really emphasize on absences and discouraged students from missing school. Parents must also discourage their children from skipping school intentionally without any proper excuse. 

```{r}
ggplot(d3) +
 aes(x = failures, y = absences) +
 geom_boxplot(fill = "#0c4c8a") +
 theme_minimal()
```

The guardian of the student is also ranked higher than alcohol consumption.When a guardian is present, the student will be supervised and therefore tend to be more obedient to the rules and regulation. Therefore, the students will be more likely to study for an exam and the chances of them to drink alcohol will be lessen by a huge extent. Therefore from this observation, we can't really determine if alcohol really is damaging the students grade. 

Another factor that are in the top 5 ignoring G1,G2,G3, is family relationship. This data shows us that a bad family relationship will increase the likelihood of failures. This is an important information since it can be used in fields such as psychology to ensure that the students relationship with his or her family members is good since it might hurt their academic performances. Also, this information tells us that parents must also work hard in maintaining a good relationship with their children so that it won't take a toll on their academic performances.

Surprisingly daily alcohol consumption is the bottom five in the plot. This shows that alcohol does not really influence children failure.Factors such as age and absences plays a bigger role than daily alcohol intake. However,we can't directly eliminate and ignore this potential relationship. Based on the plot, we can see that daily alcohol intake is just above study time. This means that we should also be worried because alcohol's influence on student failures is just as much as study time if not more.However, this is still an open claim and needs further backing from multiple surverys to ensure that our data really backs this. Also, remembering from the 'confirmed' and 'rejected' table shown previously, we can see that weekend alcohol intake is not included in this list because generally people tend to party more and study less on weekends. Therefore, its influence on school failures is too small to take into account. 

```{r}
ggplot(d3) +
 aes(x = failures, y = famrel) +
 geom_boxplot(fill = "#0c4c8a") +
 theme_minimal()
```

As shown previously, Pstatus which stand for parents cohabituation is surprisingly not inside the confirmed list. This shows us that factors such as age, absences, and even alcohol has more impact than whether or not the parents are living together or not.In the other hand, paid which stands for extra classes,is inside the 'confirmed' list.Hence, this shows us the importance of having extra classes on the performance of students in school. This is sometimes underestimated since student and even parents tend to look down on the importance of extra classes. Looking at this, students must consider joining extra classes although they might have to spend a little bit of money.

#Evaluating RF
Here we will set the training and teating dataset.70% will be on the training set and 30% will be on the testing set.

```{r}
set.seed(123,sample.kind = "Rounding")
idTrain = createDataPartition(d3$failures,
                              p = 0.7,
                              list = FALSE)

train = d3[idTrain,] # 70%
test = d3[-idTrain,] # 30%

```

Using the same variables selected by Boruta algorithm, we will try to create our final prediction'rf_final'.

```{r}
set.seed(123,sample.kind = "Rounding")
rf_final <- randomForest(failures ~ Class + age + Medu + Fedu + Mjob + guardian + studytime + paid + higher + famrel + Dalc + absences + G1 + G2 + G3, data = train)

```


We will then try to evaluate our final model by comparing failures to our predicted value 'rf_final' and test it on our testing set.Also we will then print our evaluation 'rf_eval' to see the summary and performance of the model.

```{r}
rf_eval <- confusionMatrix(test$failures,
                           predict(rf_final,
                                   test))

rf_eval
```

**2(c).Knn model**
#2nd Model:KNN

We will first create a list to compare k and accuracy and use the same variables decided by the boruta algorithm that we also use in the random forest model.
```{r}
results = list(k = rep(0,100), Accur = rep(0,100))

for (i in 2:101){
  my_kknn <- kknn(failures ~ Class + age + Medu + Fedu + Mjob + guardian + studytime + paid + higher + famrel + Dalc + absences + G1 + G2 + G3, train, test, k = i)
  
  kknn_eval <- confusionMatrix(test$failures, my_kknn$fitted.values)
  
  results$k[i] = i
  results$Accur[i] = kknn_eval$overall[1]
  
}

```

Next, we will try to find the value of 'K' by generating a plot that visualizes the distribution of the values of 'K' and find the best value. This is an important step because we always want to improve our model in any way possible.Randomly selecting a number will not produce the most effective model that we can. This is important especially knowing the fact that we can't produce a 100% accurate model, but we can try to maximise it to be as close as possible. Genereally, a lower value of K will lower the error rate, however, it is not the same for validation error. That is why we must pick the best value of k. There are different ways to get the value of k and this is just one of them.Here we will use cross validation where we will try to predict the best value of K from the training dataset. The general formula of K is K=sqrt(N) where N is the number of samples in the training dataset.

```{r}
# K vs Accuracy
plot(results$k[-1], results$Accur[-1], type="l",
     xlab="k", ylab="Accuracy")

best.K = results$k[which.max(results$Accur)]
```

Next, we will then input the best value of k,which is encoded in 'best.k' to evaluate our final knn model 'best_kknn'. 

```{r}
best_kknn <- kknn(failures ~ Class + age + Medu + Fedu + Mjob + guardian + studytime + paid + higher + famrel + Dalc + absences + G1 + G2 + G3, train, test, k = best.K)

(best_kknn_eval <- confusionMatrix(test$failures, best_kknn$fitted.values))

```


#Results

```{r}
rf_eval #random forest results
best_kknn_eval #knn results
```
**3(a).Random forest results**
Accuracy : 0.8558    
Sensitivity :  0.8817     
Specificity : 0.6364  


**3(b).KNN results**
Accuracy : 0.859  
Sensitivity : 0.8741       
Specificity : 0.6923  
 


**3(c).Summary of results**


Accuracy,sensitivity, and specificity are three important variables to look at when observing binary outcomes. To really see this, we need to first understand the idea of confusion matrix which we conduct with the confusionMatrix formula in R (as shown earlier).To visualize and simpify how the confusion matrix works, below is a table to describe it. 

```
 __________________________________________________________
| O  |                    Test Indicator                   |                 
| u  |                 |        No        |      Yes       |
| t  |_____________________________________________________|
| c  |       No        |  True Negative   | False Positive |
| o  |_____________________________________________________|
| m  |       Yes       |  False Negative  | True Positive  |
| e  |                 |                  |                |
|____|_____________________________________________________|
```


The test indicator is the predictions that we make with the model. In this case, 'Yes' means that the students do fail. "No" means that the students pass.The outcome is the actual event or what actually happens. True Negative and True Positive are generally what we want since this means that both the prediction and the actual events matches(Both "No" or Both "Yes"). However, False Positive and False Negative are what we want to avoid since it means that either our test predicts that the student fail when they dont or they predict that the student fail when they pass.Accuracy is a measure of the True Positive and True Negative of the confusion matrix. When the model predicts mostly True Positive and True Negative, then the model will be more accurate.The formula of accuracy is 

Accuracy = (TN+TP)/(TN+TP+FN+FP)

As we can see above, the KNN model is slightly better in producing accurate prediction by 0.004 or 0.4%.But, measuring the effectiveness of the model as a whole, it is not enough to conclude based on accuracy only. 

Sensitivity is defined as the proportion of positives that is predicted to be positives.In our case, the higher the sensitivity,the less likely for it to predict students failing when they don't.The formula os sensitivity is as follows: 

Sensitivity = TP/(TP + FN). 

In the other hand, specificity is the proportion of negatives that is predicted as negatives.The higher the sensitivity, the less likely for the result to predict students failing when they actually pass.The formula is given by:

Specificity = TN/(TN + FP)

Although the data shows that KNN is slightly better in accuracy than the random forest model, we can see that the random forest model has a higher percentage of sensitivity. However, Knn has a higher level of specificity. Taking into account all of these factors and acknowledging the purpose of this project, we can decide that the random forest is the better model due to the higher level of sensitivity. Since we are trying to predict the number of failures(Positive), it is better for us to look at sensitivity which measures the level of positives while minimizing the level of false negative.


\newpage

```{r}
#Conclusion
```

**4(a).Summary of findings**
According to the data and results we get, both the KNN and the randomforest model shows very similar level of accuracy. However, we can agree that the random forest model is better due to the higher level of sensitivity compared to the KNN algorithm. Also, from our predictions we can see that alcohol is not the strongest predictor of students failure. Although it is true that daily alcohol average is somewhat related to students failures, other categories such as age, absences,and family relationship has greater influences on student failures.This is congruent with our hypothesis which predicts that there will be other more important factors than alcohol. However, factors such as age and guardian can undermine the effect of alcohol mainly because as the students grow older, they might start to drink alcohol as they pass their legal age which may not be accurately reflected in our data. Also, the presence of a guardian will reduce the chances of underage drinking or alcohol consumption in general. So, there might be bigger influences of alcohol that we need to further research. 

The results shows that parent cohabituation is indeed not a good determinant of students failures.Hence, this proves that the second hypothesis is unsubstantiated. This is an interesting thing since parents status(divorced or together) might seem to be an important factor that might influence the student psychologically. However, it is not backed by the data that we try to analyze. Maybe, parents cohabituation might influence daily alcohol consumption more than it influences student failures, but that is another case of research. 

Another thing that we found is that travel time is not related to students failures as expected from our hypothesis. However, our hypothesis is wrong due to the fact that reason to go to school is not that important to student failures like what we expected. A student's reason to go to school is the foundation of their motivation.Lack of reason to go to school will most likely make the students 'lazier' and hence lower their overall performance. However, this is not the case. 

**4(b).Potential impact**

The findings on the report will be beneficial to parents and headmasters since this report can give them an understanding that as students age, they are more likely to receive lower grades. Hence, both parents and headmasters can try to minimize this by adjusting and modifying their methods and plans. For example, the school might want to encourage students to take extra classes since as supported by the data, there is a correlation between extra class and failures too. Also, parents might want to try to figure out why their children's score is diminishing. 

The findings might also help in encouraging schools and universities to require a guardian for students below 18,especially when living in other countries alone. This might help them to perform better academically and also maintain a healthier lifestyle free from alcohol. 

Parents should also know the importance of their role in their children's academic performance since mothers and fathers education is a pretty strong determinant of student failures. This implies that parents should interfere, if needed, to set up tutors or teach their children when applicable. 

This information might also be beneficial for students since it is shown that studytime is indeed important. Therefore, a student must not underestimate the impact of study time to their final output grades. This is somewhat backed by the correlation coefficient between failures and study time which shows a negative relationship. 
```{r,echo=FALSE}
cor(d3$studytime,as.numeric(d3$failures))
```
Also, increasing the final grades (G3) is really important to reduce failures. Hence, the hours spent on studying is really important. 

```{r,echo=FALSE}
cor(d3$G3,as.numeric(d3$failures))
```


**4(c).Limitations**


#Random forest[^4]

The random forest model is complex due to the number of trees that varies. Also, it reuqires a lot of time to train the dataset. 

#KNN[^5]

The limitation of KNN is that it is inefficient because the whole training set is processed for every prediction. 

In general, both our model's limitation is that we cant produce a model with 100% accuracy.There will still be a false positives or false negatives.Also, we have to make several models with various levels of training and testing dataset in oder to find the model that generates the best accuracy.

**4(d).Future work**

After analyzing the results in this model, we can build on the insights gained by trying to find if these relationships and results exists in other similar datasets. We should also work with larger datasets to prove if our model is good enough since the dataset used in this project is not so big. Therefore, further research must be conducted to make our findings and reasonings more established. 
Some of the research that we can build up on is to investigate why mothers job influences failures, but fathers job does not. Another research might be predicting the number of alcohol consumption based on factors such as family relationship, parents cohabituation and weekend alcohol consumption. 

#5.Credits(Citation)


**5(a). Dataset owner and conrtibutors**
Dataset source:
```
Citation: 
P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.

Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

BiBTeX citation:
@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

MLA Citation:

Cortez, P, and A Silva. “Student Alcohol Consumption.” Kaggle, UCI Machine Learning, 19 Oct. 2016, www.kaggle.com/uciml/student-alcohol-consumption.

Kumar, Naresh. “Advantages and Disadvantages of Random Forest Algorithm in Machine Learning.” Advantages and Disadvantages of Random Forest Algorithm in Machine Learning, theprofessionalspoint.blogspot.com/2019/02/advantages-and-disadvantages-of-random.html.

MLNerds. “How Does KNN Algorithm Work ? What Are the Advantages and Disadvantages of KNN ?” Ace the Data Science Interview!, 14 Feb. 2019, machinelearninginterview.com/topics/machine-learning/how-does-knn-algorithm-work-what-are-the-advantages-and-disadvantages-of-knn/.

Tutorialspoint. “KNN Algorithm - Finding Nearest Neighbors.” Tutorialspoint, www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_knn_algorithm_finding_nearest_neighbors.htm. 

Yiu, Tony. “Understanding Random Forest.” Understanding Random Forest, Towards Data Science, 14 Aug. 2019, towardsdatascience.com/understanding-random-forest-58381e0602d2.

[^1]:https://www.kaggle.com/uciml/student-alcohol-consumption
[^2]:https://towardsdatascience.com/understanding-random-forest-58381e0602d2
[^3]:https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_knn_algorithm_finding_nearest_neighbors.htm
[^4]:http://theprofessionalspoint.blogspot.com/2019/02/advantages-and-disadvantages-of-random.html
[^5]:https://machinelearninginterview.com/topics/machine-learning/how-does-knn-algorithm-work-what-are-the-advantages-and-disadvantages-of-knn/
```
\newpage
```{r}
print("Operating System:")
version
```
####END####